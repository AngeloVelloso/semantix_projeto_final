{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1 - Enviar os dados para o hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBS: Arquivos baixados no SO em \"data/\"\n",
    "\n",
    "> OBS: Realizao no bash\n",
    "    \n",
    "```bash\n",
    "sudo docker exec namenode bash -c \"mkdir /projeto_semantix\"\n",
    "sudo docker cp data/. namenode:/projeto_semantix\n",
    "\n",
    "sudo docker exec namenode bash -c \"hdfs dfs -mkdir -p /projeto_semantix/data\"\n",
    "sudo docker exec namenode bash -c \"hdfs dfs -moveFromLocal /projeto_semantix/*.csv /projeto_semantix/data/\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2 - Otimizar todos os dados do hdfs para uma tabela Hive particionada por município"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBS: Realizado no bash\n",
    "\n",
    "```bash\n",
    "sudo docker exec -it hive-server bash\n",
    "beeline -u jdbc:hive2://localhost:10000\n",
    "```\n",
    "\n",
    "> OBS: Realizado no beeline CLI\n",
    "\n",
    "```sql\n",
    "create database projeto_semantix;\n",
    "use projeto_semantix;\n",
    "\n",
    "create external table landing_painel_covidbr (\n",
    "    regiao string,\n",
    "    estado string,\n",
    "    municipio string,\n",
    "    coduf tinyint,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data date,\n",
    "    semanaEpi tinyint,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado int,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitada tinyint\n",
    ")\n",
    "row format delimited\n",
    "fields terminated by ';'\n",
    "stored as textfile\n",
    "location '/projeto_semantix/data'\n",
    "tblproperties (\n",
    "    \"skip.header.line.count\"=\"1\");\n",
    "\n",
    "create table painel_covidbr_brasil (\n",
    "    regiao string,\n",
    "    estado string,\n",
    "    municipio string,\n",
    "    coduf tinyint,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data date,\n",
    "    semanaEpi tinyint,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado int,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitada tinyint\n",
    ")\n",
    "stored as orc;\n",
    "\n",
    "set hive.exec.dynamic.partition = true;\n",
    "set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "\n",
    "from landing_painel_covidbr\n",
    "    insert into table painel_covidbr_brasil\n",
    "        select \n",
    "            regiao,\n",
    "            estado,\n",
    "            municipio,\n",
    "            coduf,\n",
    "            codRegiaoSaude,\n",
    "            nomeRegiaoSaude,\n",
    "            data,\n",
    "            semanaEpi,\n",
    "            populacaoTCU2019,\n",
    "            casosAcumulado,\n",
    "            casosNovos,\n",
    "            obitosAcumulado,\n",
    "            obitosNovos,\n",
    "            Recuperadosnovos,\n",
    "            emAcompanhamentoNovos,\n",
    "            interior_metropolitada\n",
    "            where regiao = 'Brasil' and year(data) = 2021;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='landing_painel_covidbr', database='projeto_semantix', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='painel_covidbr', database='projeto_semantix', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='painel_covidbr_brasil', database='projeto_semantix', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='visualizacao_item_4', database='projeto_semantix', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.setCurrentDatabase('projeto_semantix')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 (1/3) - Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| max(data)|\n",
      "+----------+\n",
      "|2021-07-06|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_spark='''\n",
    "select \n",
    "    max(data)\n",
    "from painel_covidbr_brasil\n",
    "'''\n",
    "\n",
    "spark.sql(sql_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBS: Ao contrário do que o nome sugere, os campos Recuperados Novos e Em Acompanhamento Novos apresentam os valores acumulados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_spark='''\n",
    "select \n",
    "    recuperadosnovos, emacompanhamentonovos\n",
    "from painel_covidbr_brasil\n",
    "where data >= date('2021-07-06')\n",
    "order by data\n",
    "'''\n",
    "\n",
    "df_item_4 = spark.sql(sql_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4 - Salvar a primeira visualização como tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_4.write.format(\"orc\").mode(\"overwrite\")\\\n",
    "    .option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    "    .saveAsTable(\"visualizacao_item_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 (2/3) - Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql_spark='''\n",
    "select \n",
    "    casosacumulado, casosnovos, round(casosacumulado/populacaoTCU2019*100000,2) incidencia\n",
    "from painel_covidbr_brasil\n",
    "where data >= date('2021-07-06')\n",
    "order by data\n",
    "'''\n",
    "\n",
    "df_item_5 = spark.sql(sql_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+\n",
      "|casosacumulado|casosnovos|incidencia|\n",
      "+--------------+----------+----------+\n",
      "|      18855015|     62504|   8972.29|\n",
      "+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_item_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5 - Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_5.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    "    .option(\"compression\",\"snappy\")\\\n",
    "    .saveAsTable(\"visualizacao_item_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBS: A lib do parquet está dando problema na instalação do \"BigData\" que usamos no curso, infelizmente não pude dedicar tempo suficiente para fazer o troubleshooting de infraestrutura. Cheguei a tentar substituir a versão tanto em hive/lib quanto em spark/jars, mas sem sucesso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 (3/3) - Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_spark='''\n",
    "select \n",
    "    regiao as key, cast(obitosacumulado as string) as value\n",
    "from painel_covidbr_brasil\n",
    "where data >= date('2021-07-06')\n",
    "order by data\n",
    "'''\n",
    "\n",
    "df_item_6 = spark.sql(sql_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   key| value|\n",
      "+------+------+\n",
      "|Brasil|526892|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_item_6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 6 - Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_6.write.format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'kafka:9092')\\\n",
    "    .option('topic', 'topic-item-6').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 7 - Criar a visualização pelo Spark com os dados enviados para o HDFS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_spark='''\n",
    "select \n",
    "    regiao, sum(casosacumulado) CasosAcumulados, sum(obitosacumulado) ObitosAcumulados, \n",
    "    round(sum(casosacumulado)/sum(populacaoTCU2019)*100000,2) Incidencia,\n",
    "    round(sum(obitosacumulado)/sum(populacaoTCU2019)*100000,2) Mortalidade,\n",
    "    date_format(data, 'dd/MM/yyyy') Atualizacao\n",
    "from landing_painel_covidbr\n",
    "where data >= date('2021-07-06') and codmun is null\n",
    "group by regiao, data\n",
    "'''\n",
    "\n",
    "df_item_7 = spark.sql(sql_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------------+----------+-----------+-----------+\n",
      "|      regiao|CasosAcumulados|ObitosAcumulados|Incidencia|Mortalidade|Atualizacao|\n",
      "+------------+---------------+----------------+----------+-----------+-----------+\n",
      "|         Sul|        3611041|           80705|  12046.45|     269.23| 06/07/2021|\n",
      "|Centro-Oeste|        1916619|           49207|  11760.51|     301.94| 06/07/2021|\n",
      "|       Norte|        1732815|           43845|   9401.64|     237.89| 06/07/2021|\n",
      "|      Brasil|       18855015|          526892|   8972.29|     250.73| 06/07/2021|\n",
      "|     Sudeste|        7138803|          245311|   8078.18|     277.59| 06/07/2021|\n",
      "|    Nordeste|        4455737|          107824|   7807.27|     188.93| 06/07/2021|\n",
      "+------------+---------------+----------------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_item_7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 8 - Salvar a visualização do exercício 6 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforma e prepara RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "\n",
    "rdd = df_item_7.toJSON()\n",
    "\n",
    "def addId(data):\n",
    "    j=json.dumps(data).encode('ascii', 'ignore')\n",
    "    doc_id = hashlib.sha224(j).hexdigest()\n",
    "    return (doc_id, json.dumps(data))\n",
    "\n",
    "rdd2 = rdd.map(addId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es_write_conf = {\n",
    "    \"es.nodes\" : \"localhost\",\n",
    "    \"es.port\" : \"9200\",\n",
    "    \"es.resource\" : 'semantix/_doc',\n",
    "    \"es.input.json\": \"yes\",\n",
    "    \"es.mapping.id\": \"doc_id\"\n",
    "}\n",
    "\n",
    "rdd2.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",       \n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBS: Não consegui estabelecer a conexão entre os containers Spark/ElasticSearch mesmo tentando connectá-los usando o \"docker network connect\". Infelizmente não pude dedicar tempo suficiente para fazer o troubleshooting de infraestrutura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 9 - Criar um dashboard no Elastic para visualização dos novos dados enviados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> OBS: Ficou prejudicado pelo problema de conexão acima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
